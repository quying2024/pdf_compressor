```markdown
> **历史文档说明**: 本文档描述的是v1.0版本的架构设计。
> 
> **v2.0重大更新** (2025-10-18):
> - 压缩算法完全重构：采用二分双向搜索算法
> - 拆分算法完全重构：采用纯物理拆分策略
> - 性能提升77%-99%
> - 详细信息请参阅:
>   - [`docs/算法实现说明.md`](./算法实现说明.md)
>   - [`docs/策略分析报告.md`](./策略分析报告.md)

当然，遵照您的要求，我们立即开始项目的迭代开发。

这是项目的第一版实现，它构建了完整的核心架构和逻辑，并包含了详细的日志记录功能，以便于后续的调试和功能迭代。该架构将功能清晰地划分到不同的模块中，完全遵循了研究报告中提出的设计蓝图。

### **项目架构**

以下是为您搭建的项目文件结构：

pdf\_compressor/  
├── main.py                 \# 应用程序主入口和命令行接口  
├── orchestrator.py         \# 核心业务流程调度器  
├── compressor/  
│   ├── \_\_init\_\_.py  
│   ├── pipeline.py         \# 封装DAR（解构-分析-重建）三阶段流程  
│   ├── strategy.py         \# 实现分层压缩策略与迭代优化算法  
│   ├── splitter.py         \# 实现基于qpdf的PDF拆分逻辑  
│   └── utils.py            \# 提供通用辅助函数（日志、命令执行、文件操作）  
└── logs/  
    └── process.log         \# 日志文件输出目录（自动创建）

---

### **代码文件**

下面是每个文件的详细代码实现。您可以将它们保存到对应的文件中，然后直接运行。

#### **compressor/utils.py**

这个模块提供了整个项目所需的底层辅助功能。

Python

\# compressor/utils.py

import logging  
import os  
import subprocess  
import sys  
from pathlib import Path

LOG\_DIR \= "logs"

def setup\_logging():  
    """配置日志记录器，同时输出到控制台和文件。"""  
    log\_dir \= Path(LOG\_DIR)  
    log\_dir.mkdir(exist\_ok=True)  
    log\_file \= log\_dir / "process.log"

    \# 防止重复添加处理器  
    if logging.getLogger().hasHandlers():  
        logging.getLogger().handlers.clear()

    logging.basicConfig(  
        level=logging.INFO,  
        format\="%(asctime)s \- %(levelname)s \- \[%(module)s:%(lineno)d\] \- %(message)s",  
        handlers=  
    )

def get\_file\_size\_mb(file\_path):  
    """获取文件大小，单位为MB。"""  
    try:  
        return os.path.getsize(file\_path) / (1024 \* 1024)  
    except FileNotFoundError:  
        logging.error(f"文件未找到: {file\_path}")  
        return 0

def run\_command(command, cwd=None):  
    """  
    执行一个外部命令行命令。

    Args:  
        command (list): 命令及其参数的列表。  
        cwd (str, optional): 命令执行的工作目录。

    Returns:  
        bool: 命令是否成功执行。  
    """  
    command\_str \= ' '.join(command)  
    logging.info(f"执行命令: {command\_str}")  
    try:  
        result \= subprocess.run(  
            command,  
            check=True,  
            capture\_output=True,  
            text=True,  
            encoding='utf-8',  
            errors='ignore',  
            cwd=cwd  
        )  
        if result.stdout:  
            logging.debug(f"命令输出:\\n{result.stdout}")  
        if result.stderr:  
            logging.warning(f"命令标准错误输出:\\n{result.stderr}")  
        return True  
    except subprocess.CalledProcessError as e:  
        logging.error(f"命令执行失败: {command\_str}")  
        logging.error(f"返回码: {e.returncode}")  
        logging.error(f"标准输出:\\n{e.stdout}")  
        logging.error(f"标准错误:\\n{e.stderr}")  
        return False  
    except FileNotFoundError:  
        logging.error(f"命令未找到: {command}。请确保该工具已安装并在系统PATH中。")  
        return False

#### **compressor/pipeline.py**

此模块封装了与外部工具交互的“解构-分析-重建”流程。

Python

\# compressor/pipeline.py

import logging  
import glob  
from pathlib import Path  
from. import utils

def deconstruct\_pdf\_to\_images(pdf\_path, temp\_dir, dpi):  
    """  
    使用 pdftoppm 将 PDF 转换为 TIFF 图像序列。  
    返回生成的图像文件路径列表。  
    """  
    logging.info(f"阶段1 \[解构\]: 开始将 {pdf\_path.name} 转换为图像 (DPI: {dpi})...")  
    output\_prefix \= temp\_dir / "page"  
    command \= \[  
        "pdftoppm",  
        "-tiff",  
        "-r", str(dpi),  
        str(pdf\_path),  
        str(output\_prefix)  
    \]  
    if not utils.run\_command(command):  
        logging.error("PDF解构失败。")  
        return None  
      
    image\_files \= sorted(glob.glob(f"{output\_prefix}\-\*.tif"))  
    if not image\_files:  
        logging.error("未生成任何图像文件。")  
        return None  
          
    logging.info(f"成功生成 {len(image\_files)} 页图像。")  
    return \[Path(f) for f in image\_files\]

def analyze\_images\_to\_hocr(image\_files, temp\_dir):  
    """  
    使用 tesseract 对图像进行 OCR，生成并合并 hOCR 文件。  
    返回合并后的 hOCR 文件路径。  
    """  
    logging.info(f"阶段2 \[分析\]: 开始对 {len(image\_files)} 张图像进行 OCR...")  
    hocr\_files \=  
    for i, img\_path in enumerate(image\_files):  
        output\_prefix \= temp\_dir / img\_path.stem  
        command \= \[  
            "tesseract",  
            str(img\_path),  
            str(output\_prefix),  
            "-l", "chi\_sim",  \# 假设为简体中文  
            "hocr"  
        \]  
        if not utils.run\_command(command):  
            logging.error(f"对图像 {img\_path.name} 的 OCR 失败。")  
            return None  
        hocr\_files.append(Path(f"{output\_prefix}.hocr"))  
        logging.info(f"完成 OCR: {i+1}/{len(image\_files)}")

    \# 合并所有 hocr 文件  
    combined\_hocr\_path \= temp\_dir / "combined.hocr"  
    logging.info(f"合并 hOCR 文件到 {combined\_hocr\_path}...")  
    try:  
        with open(combined\_hocr\_path, 'wb') as outfile:  
            for hocr\_file in hocr\_files:  
                with open(hocr\_file, 'rb') as infile:  
                    outfile.write(infile.read())  
    except IOError as e:  
        logging.error(f"合并 hOCR 文件时出错: {e}")  
        return None

    logging.info("hOCR 文件合并成功。")  
    return combined\_hocr\_path

def reconstruct\_pdf(image\_files, hocr\_file, temp\_dir, params, output\_pdf\_path):  
    """  
    使用 recode\_pdf 重建 PDF。  
    """  
    logging.info(f"阶段3 \[重建\]: 使用参数 {params} 重建 PDF...")  
      
    \# recode\_pdf 需要一个 glob 模式  
    image\_stack\_glob \= str(temp\_dir / "page-\*.tif")

    command \= \[  
        "recode\_pdf",  
        "--from-imagestack", image\_stack\_glob,  
        "--hocr-file", str(hocr\_file),  
        "--dpi", str(params\['dpi'\]),  
        "--bg-downsample", str(params\['bg\_downsample'\]),  
        "--mask-compression", "jbig2",  
        "-o", str(output\_pdf\_path)  
    \]  
      
    if not utils.run\_command(command):  
        logging.error("PDF 重建失败。")  
        return False  
          
    logging.info(f"PDF 重建成功，输出至 {output\_pdf\_path}")  
    return True

#### **compressor/strategy.py**

此模块定义了核心的压缩策略和迭代逻辑。

Python

\# compressor/strategy.py

import logging  
import tempfile  
from pathlib import Path  
from. import utils, pipeline

\# 定义不同层级的压缩策略  
STRATEGIES \= {  
    1: {  \# 2MB \<= S \< 10MB  
        "name": "高质量压缩",  
        "params\_sequence": \[  
            {'dpi': 300, 'bg\_downsample': 1},  
            {'dpi': 300, 'bg\_downsample': 2},  
            {'dpi': 300, 'bg\_downsample': 3},  
            {'dpi': 250, 'bg\_downsample': 2},  
            {'dpi': 200, 'bg\_downsample': 1},  
        \]  
    },  
    2: {  \# 10MB \<= S \< 50MB  
        "name": "平衡压缩",  
        "params\_sequence": \[  
            {'dpi': 300, 'bg\_downsample': 2},  
            {'dpi': 300, 'bg\_downsample': 3},  
            {'dpi': 300, 'bg\_downsample': 4},  
            {'dpi': 250, 'bg\_downsample': 3},  
            {'dpi': 200, 'bg\_downsample': 2},  
            {'dpi': 150, 'bg\_downsample': 1},  
        \]  
    },  
    3: {  \# S \>= 50MB  
        "name": "极限压缩",  
        "params\_sequence": \[  
            {'dpi': 200, 'bg\_downsample': 3},  
            {'dpi': 200, 'bg\_downsample': 4},  
            {'dpi': 200, 'bg\_downsample': 5},  
            {'dpi': 150, 'bg\_downsample': 2},  
            {'dpi': 150, 'bg\_downsample': 3},  
        \]  
    }  
}

def determine\_tier(size\_mb):  
    """根据文件大小确定处理层级。"""  
    if 2 \<= size\_mb \< 10:  
        return 1  
    if 10 \<= size\_mb \< 50:  
        return 2  
    if size\_mb \>= 50:  
        return 3  
    return 0 \# 小于2MB或无效值

def run\_iterative\_compression(pdf\_path, output\_dir, target\_size\_mb):  
    """  
    执行迭代压缩流程。  
    返回 (bool, Path): (是否成功, 输出文件路径)  
    """  
    original\_size\_mb \= utils.get\_file\_size\_mb(pdf\_path)  
    tier \= determine\_tier(original\_size\_mb)  
      
    if tier \== 0:  
        logging.warning(f"文件 {pdf\_path.name} 大小不符合压缩范围，跳过。")  
        return False, None

    strategy \= STRATEGIES\[tier\]  
    logging.info(f"文件 {pdf\_path.name} (大小: {original\_size\_mb:.2f}MB) 应用策略: 层级 {tier} ({strategy\['name'\]})")

    for i, params in enumerate(strategy\['params\_sequence'\]):  
        logging.info(f"--- 第 {i+1}/{len(strategy\['params\_sequence'\])} 次尝试: DPI={params\['dpi'\]}, BG-Downsample={params\['bg\_downsample'\]} \---")  
          
        with tempfile.TemporaryDirectory() as temp\_dir\_str:  
            temp\_dir \= Path(temp\_dir\_str)  
            output\_pdf\_path \= temp\_dir / f"output\_{pdf\_path.stem}.pdf"

            \# 1\. 解构  
            image\_files \= pipeline.deconstruct\_pdf\_to\_images(pdf\_path, temp\_dir, params\['dpi'\])  
            if not image\_files: continue

            \# 2\. 分析  
            hocr\_file \= pipeline.analyze\_images\_to\_hocr(image\_files, temp\_dir)  
            if not hocr\_file: continue

            \# 3\. 重建  
            if not pipeline.reconstruct\_pdf(image\_files, hocr\_file, temp\_dir, params, output\_pdf\_path):  
                continue

            \# 4\. 检查大小  
            result\_size\_mb \= utils.get\_file\_size\_mb(output\_pdf\_path)  
            logging.info(f"压缩结果大小: {result\_size\_mb:.2f}MB (目标: \< {target\_size\_mb}MB)")

            if result\_size\_mb \<= target\_size\_mb:  
                final\_path \= output\_dir / f"{pdf\_path.stem}\_compressed.pdf"  
                output\_pdf\_path.rename(final\_path)  
                logging.info(f"成功！文件已压缩并保存至: {final\_path}")  
                return True, final\_path

    logging.warning(f"所有压缩尝试均失败，无法将 {pdf\_path.name} 压缩至目标大小。")  
    return False, None

#### **compressor/splitter.py**

此模块负责在压缩失败时执行PDF拆分。

Python

\# compressor/splitter.py

import logging  
import math  
from pathlib import Path  
from. import utils, strategy

def get\_pdf\_page\_count(pdf\_path):  
    """使用 \`pdfinfo\` 获取PDF的总页数。"""  
    command \= \["pdfinfo", str(pdf\_path)\]  
    \# 我们需要自己运行这个命令来捕获输出  
    try:  
        result \= subprocess.run(command, check=True, capture\_output=True, text=True, encoding='utf-8')  
        for line in result.stdout.splitlines():  
            if line.startswith("Pages:"):  
                return int(line.split(":").\[1\]strip())  
    except (subprocess.CalledProcessError, FileNotFoundError) as e:  
        logging.error(f"获取 {pdf\_path.name} 的页数失败: {e}")  
    return 0

def split\_pdf(pdf\_path, output\_path, start\_page, end\_page):  
    """  
    使用 qpdf 拆分PDF文件。  
    """  
    logging.info(f"正在拆分 {pdf\_path.name}: 页码 {start\_page}\-{end\_page} \-\> {output\_path.name}")  
    command \= \[  
        "qpdf",  
        str(pdf\_path),  
        "--pages",  
        ".",  \# 代表输入文件  
        f"{start\_page}\-{end\_page}",  
        "--",  
        str(output\_path)  
    \]  
    return utils.run\_command(command)

def run\_splitting\_protocol(pdf\_path, output\_dir, args):  
    """  
    执行拆分协议。  
    """  
    logging.info(f"为 {pdf\_path.name} 启动应急拆分协议...")  
    total\_pages \= get\_pdf\_page\_count(pdf\_path)  
    if total\_pages \== 0:  
        logging.error("无法获取页数，拆分中止。")  
        return

    \# 启发式算法确定初始拆分数量  
    original\_size\_mb \= utils.get\_file\_size\_mb(pdf\_path)  
    \# 假设一个25MB的块比较有希望被压缩到2MB  
    initial\_k \= min(args.max\_splits, math.ceil(original\_size\_mb / 25))  
    if initial\_k \< 2: initial\_k \= 2

    for k in range(initial\_k, args.max\_splits \+ 1):  
        logging.info(f"尝试拆分为 {k} 部分...")  
        pages\_per\_split \= math.ceil(total\_pages / k)  
          
        all\_parts\_successful \= True  
        split\_files \=

        for i in range(k):  
            start\_page \= i \* pages\_per\_split \+ 1  
            end\_page \= min((i \+ 1) \* pages\_per\_split, total\_pages)  
              
            if start\_page \> total\_pages:  
                break

            part\_path \= output\_dir / f"{pdf\_path.stem}\_part{i+1}.pdf"  
              
            \# 1\. 拆分  
            if not split\_pdf(pdf\_path, part\_path, start\_page, end\_page):  
                logging.error(f"拆分第 {i+1} 部分失败。")  
                all\_parts\_successful \= False  
                break  
              
            \# 2\. 对拆分出的部分进行压缩  
            logging.info(f"对拆分出的文件 {part\_path.name} 进行压缩...")  
            \# 对拆分后的文件，我们直接使用最激进的策略  
            \# 注意：这里为了简化，直接调用了迭代压缩，但实际可以传入一个更激进的策略  
            success, compressed\_path \= strategy.run\_iterative\_compression(  
                part\_path, output\_dir, args.target\_size  
            )

            if success:  
                split\_files.append(compressed\_path)  
                part\_path.unlink() \# 删除未压缩的拆分文件  
            else:  
                logging.error(f"压缩第 {i+1} 部分失败。增加拆分数量...")  
                all\_parts\_successful \= False  
                \# 清理已生成的该轮次文件  
                part\_path.unlink(missing\_ok=True)  
                for f in split\_files:  
                    f.unlink(missing\_ok=True)  
                break  
          
        if all\_parts\_successful:  
            logging.info(f"成功将 {pdf\_path.name} 拆分为 {k} 部分并全部压缩成功！")  
            return

    logging.error(f"拆分协议失败：即使拆分为 {args.max\_splits} 部分，也无法完成压缩。")

#### **orchestrator.py**

此模块是业务流程的核心，它决定了对一个文件应该执行何种操作。

Python

\# orchestrator.py

import logging  
from pathlib import Path  
from compressor import utils, strategy, splitter

def process\_file(file\_path, args):  
    """  
    处理单个PDF文件的总入口。  
    """  
    logging.info(f"================== 开始处理文件: {file\_path.name} \==================")  
      
    try:  
        original\_size\_mb \= utils.get\_file\_size\_mb(file\_path)  
        if original\_size\_mb \< args.target\_size:  
            logging.info(f"文件大小 ({original\_size\_mb:.2f}MB) 已满足要求，跳过。")  
            \# 可选：如果需要，可以将文件复制到输出目录  
            \# import shutil  
            \# shutil.copy(file\_path, args.output\_dir)  
            return

        \# 运行迭代压缩  
        success, \_ \= strategy.run\_iterative\_compression(  
            file\_path,  
            Path(args.output\_dir),  
            args.target\_size  
        )

        if not success and args.allow\_splitting:  
            logging.info(f"压缩失败，且已启用拆分功能。启动拆分协议...")  
            splitter.run\_splitting\_protocol(file\_path, Path(args.output\_dir), args)  
        elif not success:  
            logging.warning(f"压缩失败，并且未启用拆分功能。")

    except Exception as e:  
        logging.critical(f"处理文件 {file\_path.name} 时发生意外错误: {e}", exc\_info=True)  
    finally:  
        logging.info(f"================== 文件处理结束: {file\_path.name} \==================\\n")

#### **main.py**

这是程序的入口点，负责解析命令行参数并启动处理流程。

Python

\# main.py

import argparse  
import logging  
from pathlib import Path  
from compressor import utils  
import orchestrator

def main():  
    """主函数：解析参数并分发任务。"""  
    utils.setup\_logging()  
      
    parser \= argparse.ArgumentParser(  
        description="基于 archive-pdf-tools 的PDF自动化压缩与拆分工具。",  
        formatter\_class=argparse.RawTextHelpFormatter  
    )  
    parser.add\_argument(  
        "--input",  
        required=True,  
        help\="输入的源路径，可以是一个PDF文件或一个包含PDF文件的目录。"  
    )  
    parser.add\_argument(  
        "--output-dir",  
        required=True,  
        help\="处理后文件的存放目录。"  
    )  
    parser.add\_argument(  
        "--target-size",  
        type\=float,  
        default=2.0,  
        help\="目标文件大小，单位为MB。默认值为 2.0。"  
    )  
    parser.add\_argument(  
        "--allow-splitting",  
        action="store\_true",  
        help\="如果提供此参数，则允许在压缩失败时启用拆分功能。"  
    )  
    parser.add\_argument(  
        "--max-splits",  
        type\=int,  
        default=4,  
        choices=\[2, 3, 4\],  
        help\="允许的最大拆分数量。默认值为 4。"  
    )  
      
    args \= parser.parse\_args()  
      
    input\_path \= Path(args.input)  
    output\_path \= Path(args.output\_dir)  
      
    \# 确保输出目录存在  
    output\_path.mkdir(parents=True, exist\_ok=True)  
      
    logging.info(f"--- 任务开始 \---")  
    logging.info(f"输入路径: {input\_path}")  
    logging.info(f"输出目录: {output\_path}")  
    logging.info(f"目标大小: \< {args.target\_size} MB")  
    logging.info(f"允许拆分: {'是' if args.allow\_splitting else '否'}")  
    if args.allow\_splitting:  
        logging.info(f"最大拆分: {args.max\_splits} 部分")

    if not input\_path.exists():  
        logging.error(f"输入路径不存在: {input\_path}")  
        return

    if input\_path.is\_dir():  
        logging.info("输入为目录，开始扫描PDF文件...")  
        pdf\_files \= sorted(list(input\_path.glob("\*.pdf")) \+ list(input\_path.glob("\*.PDF")))  
        if not pdf\_files:  
            logging.warning("在指定目录中未找到PDF文件。")  
            return  
          
        logging.info(f"找到 {len(pdf\_files)} 个PDF文件，准备处理...")  
        for pdf\_file in pdf\_files:  
            orchestrator.process\_file(pdf\_file, args)  
              
    elif input\_path.is\_file() and input\_path.suffix.lower() \== '.pdf':  
        logging.info("输入为单个PDF文件，准备处理...")  
        orchestrator.process\_file(input\_path, args)  
    else:  
        logging.error("输入路径既不是有效的目录，也不是PDF文件。")

    logging.info("--- 所有任务已完成 \---")

if \_\_name\_\_ \== "\_\_main\_\_":  
    main()

### **如何运行**

1. **保存文件**：将上述代码块分别保存到 pdf\_compressor/ 目录及其子目录 compressor/ 中对应的文件里。  
2. **安装依赖**：确保您已按照研究报告的要求，在WSL/Ubuntu环境中安装了 poppler-utils, tesseract-ocr, tesseract-ocr-chi-sim, archive-pdf-tools, 和 qpdf。  
3. **执行命令**：打开终端，进入 pdf\_compressor/ 的上一级目录，然后运行程序。

**示例命令：**

* **处理单个大文件并允许拆分：**  
  Bash  
  python3 pdf\_compressor/main.py \--input /path/to/your/large\_document.pdf \--output-dir /path/to/output \--allow-splitting

* **处理整个文件夹内的所有PDF，使用默认2MB目标大小：**  
  Bash  
  python3 pdf\_compressor/main.py \--input /path/to/source\_folder \--output-dir /path/to/processed\_files

* **处理文件，目标设为8MB，不允许拆分：**  
  Bash  
  python3 pdf\_compressor/main.py \--input document.pdf \--output-dir./output \--target-size 8.0

所有处理过程的详细信息都将实时显示在控制台，并完整记录在 pdf\_compressor/logs/process.log 文件中，这为调试和跟踪提供了极大的便利。